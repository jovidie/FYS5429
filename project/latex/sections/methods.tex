%===============================================================================
\section{Methods}\label{sec:methods}
% Describe the methods and algorithms used, include any formulas. Explain how 
% everything is implemented, and possibly mention the structure of the 
% algorithm. Add demonstrations such as tests, selected runs and validations. 
%===============================================================================
% Dataset
\subsection{Data}\label{sssec:data}
To generate trajectories, I used the package RatInABox \cite{george:2022:ratinabox}. This allowed me to set up a 2D environment to include an exploring agent (rat), similar to experimental studies. It also allowed me to import the experimental data from Sargolini et al \cite{sargolini:2006:conjunctive} to compare with synthetic data. The dataset included a number of trajectories of a given sequence length, of features of interest. Possible features include the agents position, velocity, head direction and head velocity, among others. 

% Model
\subsection{Recurrent neural network}\label{sssec:rnn}
% FFNN
The artificial neural network (ANN) was first described by Warren McCulloch and Warren Pitts, by signal processing in the brain using an abstract approach \cite{mcculloch:1943:logical}. In recent years, neural networks have shown many use cases and have evolved into several types of networks.

In the feed-forward neural network (FFNN), the information moves through the layers in one direction. The network is said to be fully connected if each neuron in a layer is connected to all neurons in the next layer. The output of one neuron can be described as 
\begin{equation}\label{eq:ffnn}
    y = f \bigg( \sum_{i=1}^{n} w_{i} x_{i} + b_{i} \bigg) = f(z),
\end{equation}
where $f$ is a non-linear activation function and $n$ is the number of inputs received.

% RNN
When dealing with sequential data, a standard FFNN is not able to ensure the ordering of the data. A recurrent neural network (RNN) can account for the dependency within the input data. The network has recurrent connections between the hidden neurons, and produce an output at every time step. In a vanilla RNN, the output from a hidden layer at time step $t$ can be written as 
\begin{equation}\label{eq:vanilla_rnn}
    \mathbf{h}^{(t)} = f \bigg( \mathbf{Wh}^{(t-1)} + \mathbf{Ux}^{(t)} +  \mathbf{b} \bigg), 
\end{equation}
where $\mathbf{W}$ and $\mathbf{U}$ are weight matrices, $\mathbf{b}$ is a bias vector, $\mathbf{h}$ is the recurrent state vector, and $\mathbf{x}$ is the input vector.

% Pytorch function
\begin{equation}\label{eq:pytorch_rnn}
    \mathbf{h}_{t} = \text{ReLU} \bigg( x_{t} W_{ih}^{T} + b_{ih} + h_{t-1}W_{hh}^{T} +  b_{hh} \bigg)
\end{equation}

%% Activation function
To calculate the output from a neuron, the output is transformed using a non-linear activation function. I will use the ReLU function from Equation \eqref{eq:relu}, as it emphasizes a biological neuron's need for an input to reach a certain threshold in order to fire. 
\begin{equation}\label{eq:relu}
    \text{ReLU} 
    \begin{cases}
        0 \ \text{if $f < 0$} \\
        f \ \text{if $f \geq 0$}
    \end{cases}
\end{equation}

%% Loss function and backpropagation
The output of the model will correspond to positional coordinates, and the mean squared error (MSE) in Equation \eqref{eq:mse} a natural choice of loss function for a regression task. To train the model, the gradient is computed and backpropagation through time...
\begin{equation}\label{eq:mse}
    \text{MSE} = \frac{1}{N} \sum_{i=0}^{N-1} (\mathbf{y}_{i} - \mathbf{\Tilde{y}}_{i})^{2}
\end{equation}


\subsection{Tools}\label{sssec:tools}
The RNN is implemented in \verb|Python|, using \verb|PyTorch| to build the models. The trajectory data is generated using \verb|RatInABox|. I use the library \verb|matplotlib| to produce all figures, and stylize them using \verb|seaborn|.

In order to validate the implementation, I created a unit tests to check if functions such as \verb|generate_data()| returned what I needed.